\section{Retrieval}
L'attenzione passa ora alle pagine scaricate. 
Ogni documento è numerato e contiene una certa quantità di caratteri, l'obiettivo 
ora è indicizzarli.

\begin{remark}
    Tipicamente bisogna fare guessing per capire la codifica del testo, esistono 
    tabelle dedicate a questo in ogni browser.
\end{remark}

\paragraph{Segmentazione dei documenti}
Partendo da un documento di ottengono sequenze di token, in qualche modo, per esempio 
spezzando il testo agli spazi, rimuovendo le stop-words etc.
Tipicamente si applicano operazioni di normalizzazione, come troncamento, lemmatizzazione, etc.

\paragraph{Matrice termini-documenti}
Ordinando la totalità dei termini nei documenti si può ottenere una rappresentazione
matriciale con documenti sulle colonne e termini per righe. Una cella indica 
la presenza o meno di una certa parola nel testo di un documento.

Quello che si fa poi è creare la trasposta ordinata, ovvero una matrice che 
per ogni token ha una lista di interi che rappresentano i documenti in cui esso 
è contenuto.

\subsection{Codici istantanei}

Un codice è un sottoinsieme di parole binarie: $C \subseteq 2^* = \{0,1\}^*$.

\begin{definition}
    Si dice che $x$ è un prefisso di $y$, $x \preceq y$ se $\exists z \in 2^*\;t.c.\;xz = y$, ovvero se concatenato ad un altra parola binaria ottengo $y$.    
\end{definition}

\begin{definition}
    Due parole $x$ e $y$ sono confrontabili se $x \preceq y$ oppure $y \preceq x$.
\end{definition}

\begin{definition}
    Un codice $C$ si dice istantaneo se $\forall x,y \in C$, $x$ e $y$ sono inconfrontabili, ovvero privo di prefissi.
\end{definition}

\begin{definition}
    Un codice $C$ si dice completo se ogni parola binaria è confrontabile 
    con una parola $x \in C$. Ovvero, aggiungendo una qualsiasi parola binaria, 
    il codice non è più istantaneo.
\end{definition}

\paragraph{Motivazioni}
Memorizzare sequenze di interi ordinate in maniera crescente può essere fatto in maniera
efficiente memorizzando gli scarti, differenze, tra due interi consecutivi.
$$1, 6, 7, 10, 12 \longrightarrow 1, 5, 1, 3, 2$$
Queste liste potrebbero essere le righe della matrice termini-documenti.

Un'altra proprietà interessante di un codice istantaneo è l'univocità di decodifica 
scorrendo una sequenza di parole concatenate. 
Infatti, visto che nessuna parola è prefisso di un'altra, ho un solo modo di decodificare una 
sequenza di zeri e uni.

\paragraph{Ordinamento nei codici istantanei}
Due esempi di codici istantanei sono:
\begin{itemize}
    \item $k = 0^k1$
    \item $k = 1^k0$
\end{itemize}
Il secondo codice mantiene l'ordine, il secondo no.

\paragraph{Importanza della completezza}
Se un codice è istantaneo e completo si può decodificare ogni stringa binaria 
casuale in modo univoco, altrimenti no.

\subsubsection{Disuguaglianza di Kraft-Mcmillan}

\begin{definition}
    Sia $C$ un codice istantaneo, vale: 
    $$\sum_{w \in C} \frac{1}{2^{|w|}} \leq 1$$
\end{definition}
Se $C$ istantaneo, $C$ è anche completo sse $\sum_{w \in C} \frac{1}{2^{|w|}} = 1$.
Nell'interpretazione geometrica\footnote{Vedi dimostrazione} non sarebbe
libero nessun intervallo. 

\begin{remark}
    Il fatto che la sommatoria dia un valore piccolo non dice nulla su un linguaggio 
    qualsiasi, non è vera quindi l'implicazione inversa.
\end{remark}

\paragraph{Teorema interi}
Siano $t_0 \leq t_1 \leq \dots \leq t_n$ interi, se vale:
$$\sum_{i \in N} \frac{1}{2^{t_i}} \leq 1$$
Allora esiste un codice istantaneo con $n$, dove la parola i-esima ha lunghezza $t_i$.

Questo fatto rende possibile un'eventuale correzione, infatti, se avessi un codice con quelle lunghezze 
ma non fosse istantaneo, so che potrei sistemarlo.

\paragraph{Visione probabilistica}
La quantità $\frac{1}{2^{|w|}}$ si può vedere come una probabilità, visto che nel caso 
in cui un codice sia istantaneo tutte quelle quantità si sommano ad uno.

Potrei pensare di codificare probabilità maggiori a parole più corte. 
La disuguaglianza di prima, e l'algoritmo greedy associato\footnote{Non presentato qui, ma simile a Huffman}
genera il codice ottimo per una distribuzione di probabilità del tipo $\frac{1}{2}, \frac{1}{4}, \dots, \frac{1}{2^k}$.

\subsection{Esempi di codici}

\subsubsection{Codici binari}

\paragraph{Notazione binaria ridotta}
Codice binario in cui si rimuove il primo 1. Per esempio: $$1001010 \rightarrow 001010$$

\paragraph{Codice binario minimale} 
Sia $s = \lceil \log k \rceil$, se $x < 2^s - k$, esso è codificato dall'$x$-esima parola binaria
di lunghezza $s-1$, altrimenti tramite la $(x-k + 2^s)$-esima parola binaria di lunghezza $s$.



\subsubsection{Elias}

\paragraph{Codice $\gamma$}
Ogni numero è scritto in notazione binaria ridotta, ovvero in binario senza il primo uno, preceduto dalla sua rappresentazione unaria.
\begin{center}
    \begin{tabular}{|l | r | r |}
        \hline
        Numero & Codifica $\gamma$ & Unario\\
        \hline
        1 & 1 & 1\\
        2 & 010 & 01\\
        3 & 011 & 001\\ 
        4 & 00100 & 0001\\ 
        5 & 00101 & 00001\\ 
        6 & 00110 & 000001\\ 
        \dots & \dots & \dots\\
        \hline
    \end{tabular}
\end{center}

La lunghezza di $x$ nella codifica $\gamma$ è $2\lambda(x) + 1$, dove $\lambda$ è
$\lfloor \log_2 x\rfloor$.
In binario invece spenderei $\lambda(x)$, ma non è istantaneo. In unario invece 
spendo una quantità pari a $x$, che in casi in cui i numeri siano piccoli va bene, altrimenti no.

\paragraph{Codice $\delta$}
Molto simile al precedente, invece che usare l'unario, 
scrivo davanti alla rappresentazione binaria ridotta la lunghezza 
del codice $\gamma$ associato all'x-esimo numero.

Confrontando i codici visti fin'ora: 
\begin{center}
    \begin{tabular}{|l | l |}
        \hline
        Codifica & Lunghezza\\
        \hline
        Unario & $x+1 = O(x)$\\
        Elias $\gamma$ & $2\lambda (x+1) + 1$\\
        Elias $\delta$ & $2\lambda (\lambda (x+1) + 1) + 1 + \lambda(x+1)$\\ 
        \hline
    \end{tabular}
\end{center}

Il codice $\delta$ sui dice asintoticamente ottimo, visto che utilizza 
un numero logaritmico di bit più qualcosa di più piccolo. 
Nel lungo termine risparmia rispetto agli altri due codici.

\subsubsection{Golomb}

Si fissa un parametro $b$ che si dice modulo del codice. Preso un 
$x > 0$ si codifica in unario la quantità $\lfloor x/ b \rfloor$, 
seguito dalla binaria minimale di $x\;\textit{mod}\;b$.

Dato un codice, posso tornare al numero con $b\lfloor x/ b \rfloor + x\;\textit{mod}\;b$.

Fissato $b = 3$, il codice diventa 

\begin{center}
    \begin{tabular}{|l | r | r |}
        \hline
        Numero & Codifica Golomb \\
        \hline
        0 & 10\\
        1 & 110\\
        2 & 111\\ 
        3 & 010\\ 
        4 & 0110\\ 
        5 & 0111\\ 
        \dots & \dots\\
        \hline
    \end{tabular}
\end{center}

\begin{remark}
    Golomb con $b = 1$ la prima parte del codice equivale all'unario. Inoltre, questo codice ha problemi simili all'unario, 
    la prima parte infatti cresce asintoticamente come $x$, 
    nella pratica è minore, ma comunque occupa molto spazio. 
\end{remark}

\subsubsection{Codici a blocchi a lunghezza variabile}
Quello che si fa è memorizzare tanti blocchi per un singolo 
numero. Il primo bit di ogni byte contiene il fatto che 
il codice è terminato oppure no. Nel caso in cui non sia terminato 
bisogna leggere uno o più altri byte per completarlo. 

Un miglioramento è mettere tutti i bit \emph{di continuazione} 
nel primo byte, per capire fin da subito quanto proseguire.

Sio può pensare dal punto di vista logico ad una lista concatenata 
che contiene pezzi di un numero, la decodifica consiste nello scorrere 
la lista e concatenare i valori in ogni nodo.

\subsubsection{Distribuzione intesa}
Ogni codice crea implicitamente una distribuzione intesa, 
ovvero parole più corte hanno probabilità maggiore, quindi, si 
può studiare come varia questa probabilità in relazione alla lunghezza 
che assumono le parole nei vari codici.\footnote{Vedi appunti, distribuzioni intese.}

Conoscendo la distribuzione di probabilità dei dati si può utilizzare 

\subsubsection{PFOR-DELTA}
Compressione che funziona molto bene quando numeri piccoli compaiono 
molto più frequentemente di quelli grandi.

Si fissa una dimensione di blocco $B \approx 256$, la compressione avviene 
per blocchi di $B$ elementi alla volta.
Quello che si fa è scorrere gli elementi di un blocco e scegliere 
un numero di bit $b$ che rappresenti almeno una percentuale $\alpha \approx 90\%$ della totalità dei numeri del blocco. 

\paragraph{Compressione}
A questo punto scorro la lista, 
se un numero è rappresentabile in $b$ bit lo scrivo, altrimenti 
scrivo un codice di escape, per esempio tutti uni o zeri.\footnote{Necessito quindi di un valore libero per questo codice nei $b$ bit.} Accodo alla fine i numeri non rappresentabili, tutti a lunghezza $a$,
la minima per rappresentarli tutti.
All'inizio della lista inserisco il numero $b$ e il valore $a$.

\paragraph{Decompressione}
La decompressione della lista è lineare, bastano due puntatori, uno all'inizio e uno che parte 
prima dei numeri accodati.
Si ottiene una buona compressione, $b$ viene molto piccolo se tanti valori sono piccoli, inoltre nell'$alpha$-percento dei casi 
non devo accedere alla memoria accodata alla lista, che è la parte più 
costosa della decompressione.

\subsubsection{Asymmetric Numeral System}
Sono sistemi in cui cifre diverse sono scritte con numero diverso di bit, 
il che permette di scrivere numeri dove alcune cifre costano tanto, 
altre di meno.

\paragraph{Premessa}
Vorremmo che, dato un messaggio $x$, l'aggiunta di un simbolo 
che compare con probabilità $p$, crei un messaggio $x^\prime \approx \frac{x}{p}$.\\
Quindi, $\log(x^\prime) \approx \log(x) + \log(\frac{1}{p})$.
Ovvero, vorrei che, aggiungendo ad un messaggio un simbolo che compare 
frequentemente, tale messaggio si allunghi di molto poco.

La codifica binaria risponde alla richiesta precedente, dove i simboli 
sono equiprobabili.
Dato un messaggio infatti, l'aggiunta di un simbolo corrisponde a raddoppiare il valore precedente e aggiungerne uno, ovvero 
aumentare di un fattore $x/\frac{1}{2}$.

Codificando ad esempio il numero $0100110$ in decimale, quello che sto facendo sono operazioni del tipo 
$x^\prime = 2x + p$, dove $p$ è 0 oppure 1. Ciò è coerente con la premessa, 
sto aggiungendo simboli che sono equiprobabili.

\paragraph{Caso asimmetrico}
Il caso binario è semplice, codifica e decodifica si fanno 
mettendo e spostando caratteri, bisogna ora capire come fare una cosa 
simile quando la probabilità dei simboli cambia.

Siano $0,1, \dots, n-1$ simboli con una relativa probabilità associata  $p_i$.

Rappresento in maniera discreta le probabilità, considerando $2^d$ blocchi, divisi in segmenti in base alle probabilità. 
Vorrei che per ogni segmento, che ha lunghezza $f_i$, valga $f_i/2^d \approx p_i$.

Calcolo l'inizio di ogni segmento in maniera cumulativa e lo chiamo $c_i$, semplicemente sommo gli $f_j, j \leq i$.

Ogni simbolo quindi ha associato un segmento di dimensione $f_i$, lungo 
in maniera proporzionale alla sua probabilità associata.

\paragraph{Esempio}
Considero tre simboli con probabilità: $$\frac{9}{10}, \frac{1}{30}, \frac{2}{30}$$
Fisso $d$ a 10, ottengo tre blocchi lunghi $f_i$ rispettivamente: 
$$\frac{922}{1024}, \frac{34}{1024}, \frac{68}{1024}$$
Calcolo ora i $c_i$, ottengo rispettivamente: 
$$0, 922, 956$$
\begin{remark}
    Se estraggo a caso da 1024, usando i $c_i$ per capire in che segmento sono finito, 
    ottengo ogni valore quasi con le probabilità iniziali. 
\end{remark}

\paragraph{Primitive}
Definiamo ora le primitive \emph{push}, \emph{sym} e \emph{pop}:
$$\mathit{push}(x, s) = \big( \big\lfloor x/ f_s \big\rfloor << d \big)
+ c_s + \big( x\;\textit{mod}\;f_s \big)$$
Quindi, eseguo uno shift e metto nei bit più bassi quella quantità.

$$\mathit{sym}(x) = s\; | \; c_s \leq x \leq c_{s+1}$$
L'ultimo simbolo è quello associato agli ultimi $d$ bit. 

$$\mathit{pop}(x) = \langle \big(x >> d\big) \cdot f_s + \big( x\;\textit{mod}\;2^d \big) - c_s, s \rangle$$
La decodifica riporta la $x$ a prima dell'inserimento.

\subsection{Liste di affissioni, rango}
% \subsubsection{TODO: Liste di affissioni}
% \dots
\subsubsection{Riassunto processo di crawling}
Possiamo riassumere il processo di crawling come: 
\begin{enumerate}
    \item \emph{Crawling}: raccolta di pagine dal web e salvataggio
    \item \emph{Parsing}: processing dei testi, come tokenizzazione, 
    normalizzazione etc.. Ogni documento diventa quindi una sequenza di 
    token numerati. In 
    pratica si ottiene un flusso di unità da analizzare della forma 
    $$\langle\mathit{token}, \mathit{doc}, \mathit{posizione}\rangle$$
    Il tutto viene memorizzato in maniera intelligente, tramite codici 
    istantanei per ridurre lo spazio in memoria.
    Quando la memoria si riempie, scarico tutti i documenti indicizzati 
    su disco. Ottengo quindi indici inversi funzionanti per una certa 
    sequenza di documenti.
    Inoltre su disco memorizzo: 
    \begin{itemize}
        \item Liste dei termini ordinata lessicograficamente;
        \item Liste di affissioni;
        \item Lista di puntatori alle liste di affissione.
    \end{itemize}
    Successivamente fondo gli indici, che sono ordinati, con una fusione
    multivia, per ottenere l'indice totale.
\end{enumerate}

\subsubsection{Rango lessicografico efficiente}

Sono necessarie strutture efficaci per il rango lessicografico, ovvero 
per sapere l'ordine delle parole.

Si fa leva sulle strutture statiche, ovvero strutture che una volta costruite
non si cambiano. 
L'obiettivo è avere per ogni termine associato il rango, per esempio: 
\begin{equation}
    a \longrightarrow 0,
    b \longrightarrow 1,
    c \longrightarrow 2,
    d \longrightarrow 3
\end{equation}

Consideriamo ora $U$ l'insieme delle stringhe, $X \subseteq U$, i termini,
ed $f : X \longrightarrow 2^b$ la funzione che associa ad un termine il 
suo rango.
Un dizionario classico, risponde un valore se la chiave è contenuta, 
altrimenti un valore di default. Il che significa che mantiene in qualche modo 
tutte le chiavi possibili. Il che significa che un dizionario 
classico occupa almeno $$\log\bigg( \frac{|U|}{|X|} \bigg) + |X|b = \log(2^b)^{|X|}$$
Il primo termine per le chiavi, il secondo per il valore delle chiavi.

A noi interessa solo rispondere a chiavi in $X$, potremmo pensare quindi a come risparmiare spazio, occupandone solo $|X|b$, mantenendo accesso costante.

\paragraph{Idea}
Creo un sistema lineare aleatorio, lo risolviamo. 
Consideriamo $m = c|X|$ numero di variabili del sistema, dove $c > 1$. Siano ora $g, h: U \longrightarrow m$ funzioni di hash.

Per ogni chiave $x$, aggiungiamo al sistema l'equazione: 
$$V_{g(x)} \oplus  V_{h(x)} = f(x)$$

Supponiamo ora di avere una soluzione del sistema, ovvero un vettore lungo $|X|$, 
che occupa $b$ bit per ogni variabile, posso computare $f(x)$ per qualsiasi $x$, 
computando $V_{g(x)} \oplus  V_{h(x)}$.
Occupo quindi uno spazio pari a $mb = c|X|b$ più lo spazio per mantenere le funzioni 
di hash, che sono definite da qualche parametro, irrisorio rispetto al resto.

\paragraph{Risoluzione del sistema}

Costruisco un grafo dove sui nodi ho le variabili e sugli archi la variabile 
associata all'equazione.

Iterativamente efoliamo il grafo, ovvero, trovo una foglia (nodo con grado 1), sia essa 
per esempio $V_2$, collegata al nodo $V_3$.
Trovo che $V_2 = f(x) \oplus V_3$.
Stacco $V_2$, trovo un'altra foglia, proseguo così. Metto da parte quindi tante equazioni 
di quel tipo.

Prima o poi svuoto il grafo, parto dall'ultima equazione e risolvo all'indietro. 

\begin{remark}
    Questo funziona solo se il grafo è aciclico.
\end{remark}

\begin{theorem}
    Sia $m$ il numero di nodi di un grafo, $n$ il numero di archi, se vale $m \geq 2.09 n$ la probabilità che un grafo aleatorio sia aciclico tende a 1.
\end{theorem}

Il teorema precedente vale anche con un numero di nodi non troppo alto. 
Ho ottenuto quindi un modo di ottenere un grafo aciclico per far andare l'algoritmo risolutivo, 
basta usare una costante $c \geq 2.09$, fissare due funzioni e ottenere un grafo 
del sistema. Se esso presenta cicli, cambio le funzioni di hash.

\paragraph{Complicazioni}
Esistono versioni con più funzioni di hash. Se ne usassi $3$, otterrei un multigrafo, 
dove gli archi coinvolgono 3 vertici. Per quel caso la costante $c$ equivale a $1.23$.

\paragraph{Ammettere cicli}
Se si ammettono cicli bisogna ricorrere alla riduzione Gaussiana del sistema, cubica nel numero di variabili. 
Per pensare di applicarla bisogna partizionare le chiavi e risolvere sottosistemi.

\paragraph{Dire se una chiave fa parte o no di X}
Siano $S:U \longrightarrow 2^b$, $S^\prime : X \longrightarrow 2^b$. 
Creo la funzione $\bar{S^\prime}$ con la tecnica del sistema, occupando $\approx 1.23|X|b$.

Se vale $S(x) = \bar{S^\prime}(x)$ allora $x \in X$, a meno di collisioni.

\subsection{Risoluzione di interrogazioni}

L'attenzione ora passa alla risoluzione di interrogazioni a un insieme di
documenti indicizzato.

Siano ora $Q$ le query, $D$ l'insieme di documenti, vogliamo computare la seguente 
funzione: 
$$Q \times D \longrightarrow \mathbb{R}$$
che associa ad ogni documento, data una query, un punteggio.

Tra le tipologie di query posso avere ricerche ad un singolo termine, 
dove il risultato è la semantica di esso, ovvero, l'insieme dei documenti in cui il termine compare, oppure posso avere cose come unione, intersezione e negazione 
di query.

\paragraph{Query di unione}
Date più query, il risultato singolo sono insiemi di documenti, uso una fusione 
multivia\footnote{Simile al merge, ma uso un min-heap per ottenere il minimo di k liste in tempo logaritmico.} per unire i risultati.
È molto facile durante la fusione tenere traccia di quali interrogazioni hanno 
dato come risultato un certo documento, utile per effettuare ranking.

Abbiamo quindi un insieme di liste di affissione per ogni query, 
uso un'approccio pigro, ovvero, leggo il documento attuale da ogni lista, 
poi leggo l'elemento next del minimo, etc.
Questo approccio è pigro se le liste sono implementate come puntatori al prossimo.\\
Non si può fare meglio di così, ovvero, serve un numero lineare di accessi 
al next, visto che l'output contiene l'unione di tutte le liste.

\paragraph{Query di intersezione}
Consideriamo ora due query di intersezione, l'intersezione delle liste si può 
fare in tempo lineare, ma si può fare di meglio. Per esempio, 
se una fosse molto più piccola dell'altra, potrei effettuare ricerca binaria 
degli elementi della piccola nella grande.

Un'altro approccio usa ricerca esponenziale, procedo ad intervalli sempre più grandi,
appena scavalco torno all'intervallo precedente e ricomincio.
Questo approccio è vantaggioso, perché si assume che l'elemento successivo sia 
abbastanza vicino a quello corrente, cosa che capita se gli elementi della corta 
sono ben disposti in quella lunga.

Nel nostro caso non si usano questi approcci, consideriamo ora la lista corta 
che inizia con 5, quella lunga con 0, è facile capire che gli elementi della lunga
fino a quando non si incontra un 5 sono irrilevanti. Pensiamo ora alla possibilità 
di utilizzare un iteratore che non solo ha un'operazione di next, ma una di 
skip-to, che salta al maggiorante di un certo valore, in questo caso si può 
andare in tempo sub-lineare.

Nel caso in cui si abbiano $k$ liste da intersecare, si utilizza un min-heap 
per decidere la lista su cui effettuare lo skip-to, infatti, conviene far avanzare la lista più indietro, fino al massimo valore corrente. 
Se il minimo e massimo coincidono significa che esiste un intersezione.

Purtroppo nella pratica funziona meglio questo approccio: 
cerco un allineamento tra le prime due, se lo trovo allineo la terza, se non lo trovo, riallineo le prime due etc.
Quando trovo un allineamento tra le prime tre, cerco nella quarta e così via.
Il tutto funziona perché, se si ordinano gli elementi per frequenza, trovare un 
match per termini poco frequenti, ovvero match per le prime liste, farà avanzare 
le ultime di moltissimo.

\paragraph{Query di not}
Scorro la lista dei documenti, ogni volta che si incontra un buco tra gli 
elementi si emettono tutti quelli in mezzo.

\paragraph{Implementazione dello skip-to}
Mantengo in memoria un campione ogni tot elementi della lista, da decidere. 
Quando richiedo uno skip-to, ricerco in dicotomica i puntatori, poi procedo 
linearmente da quello che trovo.

\paragraph{Albero di operazioni}
Nella pratica si possono combinare unioni intersezioni e negazioni in maniera molto 
complessa, quello che si fa è, mentre si chiama next della query composta, 
tutte le sotto-query avanzano in maniera indipendente per dare in output il 
risultato complessivo.

\paragraph{Early termination}
In realtà non si completerà mai del tutto un'interrogazione, ma tramite la struttura
pigra si estraggono quanti risultati di cui ho bisogno.

\paragraph{Operatore frasale}
La query è una frase, si cercano i documenti che presentano i termini nell'ordine specificato nell'interrogazione.
L'operazione soggiacente è l'intersezione, cerco quindi documenti che hanno 
almeno tutti i termini della frase, però devo considerare anche 
la posizione dei token.
L'idea su cui si basa l'algoritmo è raccogliere le posizioni dei termini 
in un documento. 
Si ottengono quindi $k$ liste, se sottraggo alle posizioni delle liste 0, 1, 2, etc. devo risolvere una semplice intersezione. 

Si osserva comunque che l'operatore frasale potrebbe fare molto lavoro
per nulla, poichè tipicamente query frasali hanno meno risultati.

\paragraph{Operatore finestra}
La richiesta ora è trovare termini all'interno di una finestra lunga $k$, 
in qualsiasi ordine.
Si utilizza un min-heap per tenere conto della posizione dei termini 
in un documento. 

% \subsection{TODO: Punteggi endogeni}
% \dots

