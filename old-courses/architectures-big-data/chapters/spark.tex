\section{Spark}
Apache Spark è un framework open source per il calcolo distribuito.

\subsection{Elementi}
\paragraph{Resilient Distributed Dataset}
È un \emph{dataset distribuito} in memoria su cui si possono effettuare operazioni in parallelo.\\
Si può ottenere da un HDFS, oppure parallelizzando qualsiasi oggetto.\\
L'idea è quella di distribuire gli elementi tramite una funzione di hash tra i nodi 
che offrono computazione, tale distribuzione può essere forzata con un \emph{repartition}.\\
L'operazione è necessaria poiché ci si potrebbe trovare in casi dove un nodo ha molti più 
elementi degli altri, oppure per raggruppare meglio le chiavi in seguito a una join.

\paragraph{Driver}
Processo responsabile dell'esecuzione di Spark, divide l'applicazione in task piccole
computabili dai nodi worker, che eseguiranno.\\
Racchiude lo \emph{Spark Context}, che permette di utilizzare Spark, e i metadati per l'RDD.

\paragraph{Master}
È un nodo che contiene il driver, si occupa di \emph{orchestrare} il lavoro tra i \emph{nodi worker}, 
e ne monitora lo stato. Può avere un nodo di backup pronto in caso di fallimento.

\paragraph{Spark Context}
È il core di Spark, permette al driver di utilizzare il cluster. È singleton e manda segnali 
di \emph{heartbit} agli \emph{executor} per monitorarne lo stato.

\paragraph{Worker}
Nodo che si occupa della \emph{computazione}, contiene molti executors.

\paragraph{Executors}
Sono quelli che effettuano le computazioni vere e proprie. Hanno un id, ed è 
garantito il backup se falliscono.

\subsection{Funzionalità e caratteristiche}

\paragraph{Pigrizia}
Spark è pigro, non computa il risultato subito ma solo quando serve. 
È possibile quindi che un errore si verifichi solo all'esecuzione di una particolare 
riga dell'RDD.

\paragraph{Map e reduce}
Spark offre alcune funzioni di map e reduce. La prima permette di applicare una funzione
ad ogni entry dell'RDD, mentre la seconda permette di eseguire riduzioni. Le seconde devono essere
commutative, poiché non è garantito l'ordine con cui si eseguono le operazioni.

\paragraph{DAG Scheduler}
Ogni qualvolta che si effettua un'operazione su un RDD non si sta facendo altro che 
aggiungere una task al DAG Scheduler. Il suo compito è quello di trasformare un 
\emph{execution plan} logico in uno fisico, in modo da effettuare la computazione vera e propria.
Ogni passo del DAG Scheduler è chiamato \emph{Stage}.

\paragraph{Task Scheduler}
Uno stage su un sottoinsieme di righe è chiamata Task.
Una Task è lanciata dal Task Scheduler ed eseguita da uno Worker 
attraverso il Cluster manager. Il numero 
di Task è proporzionale al numero di partizioni considerate nell'RDD.

\paragraph{Shared Variables}
All'esecuzione di una map o reduce, Spark distribuisce una copia delle 
variabili delle funzioni tra tutte le righe. Questo comportamento 
può portare a problemi, nel caso di variabili molto pesanti.\\
Si introducono le shared variables, che possono essere:
\begin{itemize}
    \item \emph{Broadcast variables}: read-only, per esempio, dizionario 
    condiviso in memoria su cui fare lookup
    \item \emph{Accumulators}: write-only
\end{itemize}

\subsection{Processo}
Definiti elementi e caratteristiche di Spark l'esecuzione può essere riassunta 
in questi passi:
\begin{enumerate}
    \item Submit di un'applicazione utilizzando spark-submit utility
    \item Allocazione risorse necessarie dal Resource Manager
    \item Application master si registra al Resource Manager
    \item Spark driver manda il codice all'Application Master, convertendo il codice
    in un DAG
    \item Il Driver negozia con il Cluster Manager sulle risorse, e si creano gli Stage del 
    DAG Scheduler
    \item Gli Executors sono istanziati dagli Worker
    \item Il driver tiene traccia dello stato degli Executors e manda le task al 
    Cluster manager in base alla distribuzione dei dati
    \item L'application Master crea una Container configuration per il Node Manager
    \item È creato il primo RDD
    \item Durante l'esecuzione il driver parla con l'Application Master per monitorare
    lo stato, al termine si rilasciano le risorse
\end{enumerate}

\subsection{Spark to SQL}
La scrittura di dati da Spark a un database SQL potrebbe risultare critica e 
rallentare di molto il sistema. Infatti, la presenza di molti worker che tentano
di scrivere in parallelo non aiuta, rendendo il database un collo di bottiglia.

\paragraph{Table index}
Struttura dati che aumenta le performance in lettura di una particolare tabella del 
database, non è necessario infatti effettuare una scan completa, ma si individua 
immediatamente la riga necessaria.
In fase di inserimento, sono necessarie scritture addizionali per aggiornare l'indice.

\paragraph{Problemi}
Esistono problemi nella scrittura da Spark a SQL. In particolare, le operazioni di 
write occupano memoria, bisogna stabilire una connessione fisica e le tabelle indexed 
vanno lockate.

\paragraph{Struttura interna database}
Un database SQL è composto da data file e log file. I primi sono raggruppati in extents 
composti da pagine. Essi possono essere uniform, se ogni pagina è una data page, oppure 
mixed, se alcune pagine contengono informazioni sugli indici.

\paragraph{Esecuzione query}
All'arrivo di una query, questa viene convertita in un TDS utilizzando uno tra ODBC, 
OLEDB, SNAC, o SNI.
Successivamente, SNI sul server decapsula il TDS e passa i comandi al query parser.
Il query executor esegue la query e richiede agli access methods le pagine che sono 
coinvolte dalla query. Se si parla di query non select, interviene la logica di log e lock.
La prima logga le operazioni, la seconda effettua il lock sulle pagine per garantire ACID properties.

\paragraph{Tecniche di connessione}
Per connettersi a un dataabase esistono tre metodi:
\begin{enumerate}
    \item \emph{Connessione diretta}: si apre una connessione con una socket, bisogna 
    riaprirla ogni volta
    \item \emph{Connection pooling}: si mantengono connessioni aperte e vengono assegnate 
    ai client che le richiedono
    \item \emph{Pool fragmentation}: si creano molti pool e ogni client può usare il proprio
     pool o no, differisce dal secondo perché ogni client ha il proprio pool
\end{enumerate}

\paragraph{Creazione e update di un indice}
Se una tabella risulta indicizzata, un inserimento deve tenere conto 
di alcune cose.
Utilizzando ad esempio un B-tree+, i dati sono solo nelle foglie, le distanze tra le 
foglie non cambiano.\\
Per creare un indice i dati sono inizialmente ordinati, si sceglie una root e si 
aggiungono dati ad essa, quando questa è piena, si aggiunge un'altra root figlia e si procede.\\
Per aggiornare un indice, prima si cerca la posizione dove mettere l'informazione, 
se la pagina trovata è piena, si splitta la pagina ricorsivamente fino alla root.

\paragraph{Scrittura intermedia Data Lake}
Per risolvere il problema di scrittura in SQL da parte di worker multipli, si possono 
scrivere tutte le informazioni su un Data Lake e successivamente scriverle in bulk su SQL.\\
L'idea è quella di sfruttare il BCP, bulk copy program, per copiare tutte le righe nuove 
da datalake a SQL, e utilizzare MERGE per unirle alla vecchia tabella. La MERGE può definire 
logiche di unione.\\
L'approccio evita la creazione di dirty pages.