\section{Elasticsearch}
\begin{center}
    \emph{Why do we need search?}
\end{center}
In alcuni scenari è molto importante la ricerca, ad esempio in un Ecommerce oppure
un search engine.\\
È impossibile creare una ricerca tramite un database classico, molto probabilmente
il risultato che si vuole ottenere non corrisponde a una query singola, probabilmente
si sta facendo una ricerca fuzzy.

\subsection{Indicizzazione}
Elasticsearch è un engine distribuito di ricerca e analisi per dati di qualsiasi tipo, 
strutturati o no.\\
È basato su Apache Lucene, che offre analisi e ricerche testuali, lo estende e lo semplifica.
Offre una REST API e costituisce la base dell'EKL stack.

\paragraph{Processo}
Per popolare Elasticsearch si seguono solitamente questi passi:
\begin{enumerate}
    \item I dati si raccolgono da una sorgente
    \item Parsing e normalizzazione dei dati e indicizzazione Elastic
    \item Query sull'indice, anche molto complesse
    \item Data visualization con Kibana
\end{enumerate}

\paragraph{Indice}
Un indice Elasticsearch è una collezione di documenti che per qualche ragione sono raggruppati.
Un indice ha varie caratteristiche:
\begin{itemize}
    \item Ogni documento è un JSON
    \item Ogni indice ha un \emph{inverted index} che mantiene le parole presenti in ogni documento, 
    questo garantisce ricerche full-text molto efficienti
    \item Elasticsearch promette di riuscire ad indicizzare ogni documento in meno di un secondo
    \item Un indice è distribuito in shards, garantendo ridondanza e scalabilità
\end{itemize}

\subsection{Text preprocessing}

\paragraph{Bag of Words}
Dato un insieme di testi, si crea una lista di di parole univoche ed ordinate che compaiono in essi, 
questo sarà l'Absolute dictionary.
Ogni documento ha associato un vettore, che contiene per ogni parola dell'absolute vector
il numero di occorrenze nel testo.
Si crea quindi una matrice di documenti sulle righe e parole sulle colonne.

\paragraph{Inverted index}
L'approccio opposto al bag of word è quello di associare ad ogni parola, invece che ad ogni 
documento, i documenti in cui essa è contenuta.
La matrice quindi è inversa, le parole sono sulle righe e i documenti sulle colonne.

\paragraph{Analyzer}
Elasticsearch prima di indicizzare i testi si occupa di applicare alcune trasformazioni 
ai testi, per evitare alcuni problemi legati all'analisi testuale puntuale.

\paragraph{Lemming and stemming}
Prima di procedere a un'analisi testuale è necessario un preprocessing.
L'idea è quella di ridurre ogni parola alla sua forma base, in modo da unificare in un 
unico termine forme complesse dello stesso verbo, o singolari e plurali dei termini.
Stemming è euristico, mentre lemming è deterministico.

\paragraph{Stop words removal}
Un'altra cosa che bisogna fare prima di procedere ad un'analisi testuale è 
la rimozione delle così dette stop words, ovvero congiunzioni o termini specifici del 
linguaggio.

\paragraph{Mapping}
Un indice elastic cerca di capire il tipo degli attributi degli oggetti contenuti nell'indice.
Per ogni attributo si può poi definire come gestire l'indice, le opzioni possibili sono:
\begin{itemize}
    \item \emph{Analyzed}: si applica l'analyzer, ovvero stemming, lemming, tokenization etc.
    \item \emph{Not analyzed}: indicizza il testo così com'è
    \item \emph{No}: campo non indicizzato
\end{itemize}
Esiste poi la possibilità di avere tipi innestati, che sono appiattiti da Lucene.

\subsection{Query}
Ogni query assegna uno score ai documenti, in base a quanto matchano la query.

\paragraph{Query context e Filter context}
Le query a un indice Elasticsearch sono in json, esistono vari tipi di query, che differiscono 
per complessità. Ad ogni documento viene assegnato uno score di matching.\\
Un esempio di query è quella \emph{bool}, che permette di definire condizioni molto 
complesse, con clausole \emph{must}, \emph{must not}, \emph{should}, \emph{filter}.\\
Tutte le clausole a parte la prima fanno parte del \emph{Query context}, la seconda invece 
del \emph{Filter context}. È consigliato quindi utilizzare per query full text, condizioni 
che devono contribuire allo score nel query context, e tutto il resto del filter, il quale 
eliminerà molti documenti con altrimenti score nullo.

\subsection{Shards e scalabilità}
Elasticsearch è basato su uno storage object pattern, quindi la scalabilità orizzontale
è garantita.
L'architettura è abbastanza simile a quella di Hadoop.

\paragraph{Master node}
Esiste un nodo master che si occupa di richieste 
generali come creare indici e gestire le risorse.

\paragraph{Name node, kinda}
Non esistono veri e propri name nodes come in Hadoop, infatti, tutti i nodi contengono
informazioni su dove sono i documenti, in modo da reindirizzare le richieste verso 
il nodo che contiene le informazioni richieste.

\paragraph{Shard}
Ogni cosa è organizzata in shard, ognuno dei quali 
può essere primario o una replica di uno primario.
Ogni nodo ha multiple shard.\\
Uno shard è un search engine funzionante, e può essere utilizzato a sé.

\paragraph{Scalabilità}
Distribuendo gli shard tra i nodi si possono garantire elevate performace
e più scalabilità.

\section{NPL and semantic}
Per effettuare analisi testuali si ricorre inizialmente a un modello Bag of Words, 
come discusso a sezione precedente. Bisogna poi però trovare un modo di confrontare due 
testi sotto forma di vettore. Si introducono quindi misure di distanza e di similarità.

\subsection{Distanza e similarità}
Computando la distanza tra due vettori, dove i vettori sono bag of words, si può 
calcolare la distanza di due testi.

\paragraph{Distanza Euclidea}
La distanza tra due vettori è data da
$$
    dis(A, B) = \sqrt{\sum_{i=0}^{n} (A_i - B_i)^2}
$$
È infattibile computare la distanza euclidea per vettori molto sparsi molto lunghi.

\paragraph{Cosine similarity}
La similarità tra due vettori è data da
$$
    cos(\theta) = \frac{A \cdot B}{||A||  \cdot  ||B||} = 
    \frac{\sum_{i=0}^{n} A_i \cdot B_i}{\sqrt{\sum_{i=0}^{n} A_i}  \cdot  \sqrt{\sum_{i=0}^{n} B_i}}
$$
La computazione della cosine similarity risulta meno intensa rispetto alla distanza euclidea.
Infatti, considerando ad esempio una computazione fatta su Spark, utilizzando la 
distanza euclidea bisognerebbe mantenere tutti gli 0 nel vettore per ogni documento, infatti 
uno 0 può contribuire alla distanza nel caso in cui l'altro documento abbia valore 1. \\
Nel secondo 
caso uno 0 non contribuisce alla cosine similarity, quindi si possono mantenere, per ogni documento, 
solo i termini in esso contenuti.